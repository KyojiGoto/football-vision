//
//  ContentView.swift
//  FootballVision
//
//  Created by Kyoji Goto-Bernier on 2025-05-23.
//

import SwiftUI
import AVFoundation
import Vision

// ViewModel to manage camera setup, session, and Vision requests
class CameraViewModel: NSObject, ObservableObject, AVCaptureVideoDataOutputSampleBufferDelegate {
    @Published var isPermissionGranted = false
    @Published var detectedJoints: [VNHumanBodyPose3DObservation.JointName: SIMD3<Float>] = [:]
    @Published var detectedBalls: [CGRect] = [] // For bounding boxes of detected balls

    let session = AVCaptureSession()
    private var previewLayer: AVCaptureVideoPreviewLayer?
    private let videoDataOutput = AVCaptureVideoDataOutput()
    private let videoDataOutputQueue = DispatchQueue(label: "VideoDataOutputQueue", qos: .userInitiated, attributes: [], autoreleaseFrequency: .workItem)

    private var bodyPoseRequest = VNDetectHumanBodyPose3DRequest()
    // Object recognition request, now using VNCoreMLRequest
    private var objectRecognitionRequest: VNCoreMLRequest? // Changed to VNCoreMLRequest?

    override init() {
        super.init()
        setupObjectRecognition()
    }

    private func setupObjectRecognition() {
        // Placeholder for the Core ML model class generated by Xcode
        // If "SoccerBallDetector.mlmodel" was added to the project, Xcode would generate a class
        // typically named "SoccerBallDetector". We use this name conceptually here.
        // #warning "Replace SoccerBallDetector with your actual model class name if available."

        // Attempt to load the Core ML model
        do {
            // The following line assumes a model file "SoccerBallDetector.mlmodel" exists
            // and has generated a corresponding Swift class "SoccerBallDetector".
            // This line WILL CAUSE A COMPILE ERROR if SoccerBallDetector.mlmodel (and its class) is not in the project.
            // For the purpose of this task, we are writing the structure as if it *could* exist.
            // To make this runnable without a real model, this part would need to be commented out or
            // replaced with a conditional compilation block for a dummy model if one was provided.
            // For now, we'll assume the placeholder is for a model that might exist.
            // let model = try VNCoreMLModel(for: SoccerBallDetector().model) // This line would be used with a real model class

            // To make this code runnable for the agent without a real model,
            // we will simulate the model loading failure path.
            // In a real scenario with a model, you would uncomment the line above and handle the error.
            let modelProviderError = NSError(domain: "com.example.ModelProvider", code: -1, userInfo: [NSLocalizedDescriptionKey: "SoccerBallDetector.mlmodel and its class 'SoccerBallDetector' are not available."])
            throw modelProviderError // Simulate model loading failure

            // If model loaded successfully, create the VNCoreMLRequest
            // self.objectRecognitionRequest = VNCoreMLRequest(model: model) { [weak self] request, error in
            //     if let error = error {
            //         print("VNCoreMLRequest (Object Recognition) error: \(error.localizedDescription)")
            //         return
            //     }
            //     guard let self = self, let results = request.results as? [VNRecognizedObjectObservation] else {
            //         print("VNCoreMLRequest (Object Recognition) did not return valid results.")
            //         return
            //     }

            //     var foundBalls: [CGRect] = []
            //     for observation in results {
            //         // Assuming the custom model directly identifies soccer balls,
            //         // or provides labels we can check.
            //         // For a dedicated soccer ball detector, all observations might be balls.
            //         // If it's a broader object detector, we might need to check labels again.
            //         // For this example, let's assume any VNRecognizedObjectObservation from this model is a ball
            //         // if its confidence is high enough.
            //         if observation.confidence > 0.6 { // Example confidence for a custom model
            //             foundBalls.append(observation.boundingBox)
            //         }
            //     }
            //     DispatchQueue.main.async {
            //         self.detectedBalls = foundBalls
            //     }
            // }
            // print("Custom Core ML model for object detection loaded successfully.")

        } catch {
            print("Failed to load Core ML model (e.g., SoccerBallDetector.mlmodel): \(error.localizedDescription)")
            print("Object detection will be disabled.")
            self.objectRecognitionRequest = nil
        }
    }


    func setupSession(completion: @escaping (AVCaptureVideoPreviewLayer?) -> Void) {
        DispatchQueue.global(qos: .userInitiated).async {
            guard let device = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back) else {
                print("Failed to get back camera")
                DispatchQueue.main.async { completion(nil) }
                return
            }
            do {
                self.session.beginConfiguration() // Batch changes

                // Camera Input
                let input = try AVCaptureDeviceInput(device: device)
                if self.session.canAddInput(input) {
                    self.session.addInput(input)
                }

                // Video Data Output for Vision
                if self.session.canAddOutput(self.videoDataOutput) {
                    self.session.addOutput(self.videoDataOutput)
                    self.videoDataOutput.setSampleBufferDelegate(self, queue: self.videoDataOutputQueue)
                    self.videoDataOutput.alwaysDiscardsLateVideoFrames = true
                    self.videoDataOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_420YpCbCr8BiPlanarFullRange]
                } else {
                    print("Could not add video data output to the session")
                    self.session.commitConfiguration()
                    DispatchQueue.main.async { completion(nil) }
                    return
                }
                
                self.session.sessionPreset = .hd1920x1080 // Or another preset

                self.session.commitConfiguration() // Apply changes

                // Preview Layer (must be on main thread)
                DispatchQueue.main.async {
                    let previewLayer = AVCaptureVideoPreviewLayer(session: self.session)
                    previewLayer.videoGravity = .resizeAspectFill
                    self.previewLayer = previewLayer
                    completion(previewLayer)
                }
                
                // Start session
                if !self.session.isRunning {
                     self.session.startRunning()
                }

            } catch {
                print("Error setting up camera input or output: \(error.localizedDescription)")
                self.session.commitConfiguration() // Ensure commit even on error
                DispatchQueue.main.async { completion(nil) }
            }
        }
    }

    func requestPermission() {
        AVCaptureDevice.requestAccess(for: .video) { granted in
            DispatchQueue.main.async {
                self.isPermissionGranted = granted
                if !granted {
                    print("Camera permission denied.")
                }
            }
        }
    }

    // AVCaptureVideoDataOutputSampleBufferDelegate method
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard CMSampleBufferGetImageBuffer(sampleBuffer) != nil else {
            return
        }
        
        let imageRequestHandler = VNImageRequestHandler(cmSampleBuffer: sampleBuffer, orientation: .up, options: [:])
        var requestsToPerform: [VNRequest] = [self.bodyPoseRequest]
        
        // Conditionally add objectRecognitionRequest only if it's non-nil
        if let objectRecognitionRequest = self.objectRecognitionRequest {
            requestsToPerform.append(objectRecognitionRequest)
        }

        do {
            try imageRequestHandler.perform(requestsToPerform)
            
            // Process body pose results (existing logic)
            if let bodyPoseResults = self.bodyPoseRequest.results, let observation = bodyPoseResults.first {
                var newJoints: [VNHumanBodyPose3DObservation.JointName: SIMD3<Float>] = [:]
                let allJointNames = VNHumanBodyPose3DObservation.JointName.allCases
                for jointName in allJointNames {
                    if let recognizedPoint = try? observation.recognizedPoint(jointName), recognizedPoint.confidence > 0.1 {
                        newJoints[jointName] = recognizedPoint.position
                    }
                }
                DispatchQueue.main.async {
                    self.detectedJoints = newJoints
                }
            } else {
                DispatchQueue.main.async {
                    self.detectedJoints = [:]
                }
            }
            // Note: Object recognition results are handled in the completion handler of objectRecognitionRequest setup in init.

        } catch {
            print("Failed to perform Vision request(s): \(error.localizedDescription)")
        }
    }
}

// UIViewRepresentable to host the camera preview
struct CameraView: UIViewRepresentable {
    @ObservedObject var viewModel: CameraViewModel
    var onPreviewLayerCreated: (AVCaptureVideoPreviewLayer) -> Void

    func makeUIView(context: Context) -> UIView {
        let view = UIView()
        view.setContentCompressionResistancePriority(.defaultLow, for: .horizontal)
        view.setContentCompressionResistancePriority(.defaultLow, for: .vertical)
        return view
    }

    func updateUIView(_ uiView: UIView, context: Context) {
        // Only setup session and add preview layer once
        if viewModel.isPermissionGranted && uiView.layer.sublayers?.first(where: { $0 is AVCaptureVideoPreviewLayer }) == nil {
            viewModel.setupSession { previewLayer in
                guard let previewLayer = previewLayer else { return }
                // Ensure frame is set on the main thread after previewLayer is created
                previewLayer.frame = uiView.bounds 
                uiView.layer.addSublayer(previewLayer)
                onPreviewLayerCreated(previewLayer)
            }
        } else if let existingPreviewLayer = uiView.layer.sublayers?.first(where: { $0 is AVCaptureVideoPreviewLayer }) as? AVCaptureVideoPreviewLayer {
            // Update frame if view bounds change (e.g., on rotation)
             if existingPreviewLayer.frame != uiView.bounds {
                 existingPreviewLayer.frame = uiView.bounds
             }
        }
    }
}

struct ContentView: View {
    @StateObject private var cameraViewModel = CameraViewModel()
    @State private var previewLayerHolder: AVCaptureVideoPreviewLayer? // Use a more descriptive name

    var body: some View {
        ZStack {
            if cameraViewModel.isPermissionGranted {
                CameraView(viewModel: cameraViewModel, onPreviewLayerCreated: { layer in
                    self.previewLayerHolder = layer
                })
                .edgesIgnoringSafeArea(.all)
                .onAppear {
                    if !cameraViewModel.session.isRunning && self.previewLayerHolder != nil {
                        cameraViewModel.session.startRunning()
                    }
                }
                .onDisappear {
                    if cameraViewModel.session.isRunning {
                        cameraViewModel.session.stopRunning()
                    }
                }

                // Overlay for drawing detected poses and balls
                PoseOverlayView(
                    detectedJoints: cameraViewModel.detectedJoints,
                    detectedBalls: cameraViewModel.detectedBalls, // Pass detected balls
                    previewLayer: previewLayerHolder
                )

            } else {
                VStack {
                    Text("Camera permission is required to use this feature.")
                        .multilineTextAlignment(.center)
                        .padding()
                    Button("Request Permission") {
                        cameraViewModel.requestPermission()
                    }
                }
            }
        }
        .onAppear {
            if !cameraViewModel.isPermissionGranted {
                 cameraViewModel.requestPermission()
            }
        }
        .edgesIgnoringSafeArea(.all) // Apply to ZStack to ensure overlay also ignores it
    }
}

struct PoseOverlayView: View {
    var detectedJoints: [VNHumanBodyPose3DObservation.JointName: SIMD3<Float>]
    var detectedBalls: [CGRect] // Add property for detected balls
    var previewLayer: AVCaptureVideoPreviewLayer? // Pass the previewLayer for coordinate conversion

    // Define connections between joints for drawing lines
    let jointPairs: [(VNHumanBodyPose3DObservation.JointName, VNHumanBodyPose3DObservation.JointName)] = [
        (.neck, .head), (.neck, .spine), (.spine, .root),
        (.neck, .rightShoulder), (.rightShoulder, .rightElbow), (.rightElbow, .rightWrist),
        (.neck, .leftShoulder), (.leftShoulder, .leftElbow), (.leftElbow, .leftWrist),
        (.root, .rightHip), (.rightHip, .rightKnee), (.rightKnee, .rightAnkle),
        (.root, .leftHip), (.leftHip, .leftKnee), (.leftKnee, .leftAnkle)
    ]

    var body: some View {
        Canvas { context, size in
            guard previewLayer != nil else { return } // Removed unused previewLayer variable here

            // Draw detected body poses (existing logic)
            for (jointName, position3D) in detectedJoints {
                let projectedPoint = CGPoint(x: CGFloat(position3D.x + 0.5) * size.width, 
                                             y: CGFloat(-position3D.y + 0.5) * size.height) 

                context.fill(Path(ellipseIn: CGRect(x: projectedPoint.x - 5, y: projectedPoint.y - 5, width: 10, height: 10)), with: .color(.green))
                context.draw(Text("\(jointName.rawValue.capitalized) (\(String(format: "%.2f", position3D.z))m)") .font(.caption), at: projectedPoint + CGPoint(x:0, y:-10))
            }
            
            for pair in jointPairs {
                guard let joint1Pos3D = detectedJoints[pair.0],
                      let joint2Pos3D = detectedJoints[pair.1] else {
                    continue
                }
                 let p1 = CGPoint(x: CGFloat(joint1Pos3D.x + 0.5) * size.width, 
                                  y: CGFloat(-joint1Pos3D.y + 0.5) * size.height)
                 let p2 = CGPoint(x: CGFloat(joint2Pos3D.x + 0.5) * size.width, 
                                  y: CGFloat(-joint2Pos3D.y + 0.5) * size.height)
                var linePath = Path()
                linePath.move(to: p1)
                linePath.addLine(to: p2)
                context.stroke(linePath, with: .color(.red), lineWidth: 2)
            }

            // Draw detected balls
            for ballBoundingBox in detectedBalls {
                // VNRecognizedObjectObservation.boundingBox is normalized (0-1) with origin at bottom-left.
                // Convert to Canvas coordinates (origin top-left).
                let canvasRect = CGRect(
                    x: ballBoundingBox.origin.x * size.width,
                    y: (1 - ballBoundingBox.origin.y - ballBoundingBox.height) * size.height, // Flip Y and adjust for height
                    width: ballBoundingBox.width * size.width,
                    height: ballBoundingBox.height * size.height
                )
                context.stroke(Path(roundedRect: canvasRect, cornerRadius: 4), with: .color(.blue), lineWidth: 2)
                context.draw(Text("Ball").font(.caption), at: CGPoint(x: canvasRect.minX, y: canvasRect.minY - 10))
            }
        }
        .opacity(0.7) // Make overlay semi-transparent
    }
}


#Preview {
    ContentView()
}
