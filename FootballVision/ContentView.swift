//
//  ContentView.swift
//  FootballVision
//
//  Created by Kyoji Goto-Bernier on 2025-05-23.
//

import SwiftUI
import AVFoundation
import Vision

// ViewModel to manage camera setup, session, and Vision requests
class CameraViewModel: NSObject, ObservableObject, AVCaptureVideoDataOutputSampleBufferDelegate {
    @Published var isPermissionGranted = false
    @Published var detectedJoints: [VNHumanBodyPose3DObservation.JointName: SIMD3<Float>] = [:]
    @Published var detectedBalls: [CGRect] = [] // For bounding boxes of detected balls

    let session = AVCaptureSession()
    private var previewLayer: AVCaptureVideoPreviewLayer?
    private let videoDataOutput = AVCaptureVideoDataOutput()
    private let videoDataOutputQueue = DispatchQueue(label: "VideoDataOutputQueue", qos: .userInitiated, attributes: [], autoreleaseFrequency: .workItem)

    private var bodyPoseRequest = VNDetectHumanBodyPose3DRequest()
    // Object recognition request - will try to identify balls
    private var objectRecognitionRequest: VNRecognizeObjectsRequest!

    override init() {
        super.init()
        setupObjectRecognition()
    }

    private func setupObjectRecognition() {
        objectRecognitionRequest = VNRecognizeObjectsRequest { [weak self] request, error in
            if let error = error {
                print("Object recognition error: \(error.localizedDescription)")
                return
            }
            guard let self = self, let results = request.results as? [VNRecognizedObjectObservation] else {
                return
            }

            var foundBalls: [CGRect] = []
            for observation in results {
                // Check labels for "ball" or related terms. This is a basic check.
                // A custom model would be more accurate for "soccer ball".
                let ballLabels = ["ball", "sports ball", "soccer ball", "football"] 
                // Some generic models might identify balls as "circle" or other shapes at a distance.
                // For now, we keep it simple.
                
                let hasBallLabel = observation.labels.contains { label in
                    ballLabels.contains(where: label.identifier.lowercased().contains) && label.confidence > 0.5 // Confidence threshold
                }

                if hasBallLabel {
                    foundBalls.append(observation.boundingBox) // Normalized coordinates
                }
            }
            DispatchQueue.main.async {
                self.detectedBalls = foundBalls
            }
        }
        // For general object recognition, accuracy for specific items like soccer balls might be limited.
        // If a custom Core ML model (e.g., "SoccerBallDetector.mlmodel") were available:
        // 1. Add the .mlmodel file to the project.
        // 2. Compile it by clicking the model file and ensuring Target Membership is set.
        // 3. Load the model:
        //    guard let customModel = try? VNCoreMLModel(for: SoccerBallDetector().model) else {
        //        fatalError("Failed to load custom Core ML model for soccer ball detection.")
        //    }
        // 4. Create a VNCoreMLRequest:
        //    self.objectRecognitionRequest = VNCoreMLRequest(model: customModel) { [weak self] request, error in ... }
        // Note: `SoccerBallDetector()` would be the class generated by Xcode from the .mlmodel file.
        // Using `VNRecognizeObjectsRequest` here as a general starting point.
    }


    func setupSession(completion: @escaping (AVCaptureVideoPreviewLayer?) -> Void) {
        DispatchQueue.global(qos: .userInitiated).async {
            guard let device = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back) else {
                print("Failed to get back camera")
                DispatchQueue.main.async { completion(nil) }
                return
            }
            do {
                self.session.beginConfiguration() // Batch changes

                // Camera Input
                let input = try AVCaptureDeviceInput(device: device)
                if self.session.canAddInput(input) {
                    self.session.addInput(input)
                }

                // Video Data Output for Vision
                if self.session.canAddOutput(self.videoDataOutput) {
                    self.session.addOutput(self.videoDataOutput)
                    self.videoDataOutput.setSampleBufferDelegate(self, queue: self.videoDataOutputQueue)
                    self.videoDataOutput.alwaysDiscardsLateVideoFrames = true
                    self.videoDataOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_420YpCbCr8BiPlanarFullRange]
                } else {
                    print("Could not add video data output to the session")
                    self.session.commitConfiguration()
                    DispatchQueue.main.async { completion(nil) }
                    return
                }
                
                self.session.sessionPreset = .hd1920x1080 // Or another preset

                self.session.commitConfiguration() // Apply changes

                // Preview Layer (must be on main thread)
                DispatchQueue.main.async {
                    let previewLayer = AVCaptureVideoPreviewLayer(session: self.session)
                    previewLayer.videoGravity = .resizeAspectFill
                    self.previewLayer = previewLayer
                    completion(previewLayer)
                }
                
                // Start session
                if !self.session.isRunning {
                     self.session.startRunning()
                }

            } catch {
                print("Error setting up camera input or output: \(error.localizedDescription)")
                self.session.commitConfiguration() // Ensure commit even on error
                DispatchQueue.main.async { completion(nil) }
            }
        }
    }

    func requestPermission() {
        AVCaptureDevice.requestAccess(for: .video) { granted in
            DispatchQueue.main.async {
                self.isPermissionGranted = granted
                if !granted {
                    print("Camera permission denied.")
                }
            }
        }
    }

    // AVCaptureVideoDataOutputSampleBufferDelegate method
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard CMSampleBufferGetImageBuffer(sampleBuffer) != nil else {
            return
        }
        
        let imageRequestHandler = VNImageRequestHandler(cmSampleBuffer: sampleBuffer, orientation: .up, options: [:])
        var requestsToPerform: [VNRequest] = [self.bodyPoseRequest]
        if self.objectRecognitionRequest != nil {
             requestsToPerform.append(self.objectRecognitionRequest)
        }


        do {
            try imageRequestHandler.perform(requestsToPerform)
            
            // Process body pose results (existing logic)
            if let bodyPoseResults = self.bodyPoseRequest.results, let observation = bodyPoseResults.first {
                var newJoints: [VNHumanBodyPose3DObservation.JointName: SIMD3<Float>] = [:]
                let allJointNames = VNHumanBodyPose3DObservation.JointName.allCases
                for jointName in allJointNames {
                    if let recognizedPoint = try? observation.recognizedPoint(jointName), recognizedPoint.confidence > 0.1 {
                        newJoints[jointName] = recognizedPoint.position
                    }
                }
                DispatchQueue.main.async {
                    self.detectedJoints = newJoints
                }
            } else {
                DispatchQueue.main.async {
                    self.detectedJoints = [:]
                }
            }
            // Note: Object recognition results are handled in the completion handler of objectRecognitionRequest setup in init.

        } catch {
            print("Failed to perform Vision request(s): \(error.localizedDescription)")
        }
    }
}

// UIViewRepresentable to host the camera preview
struct CameraView: UIViewRepresentable {
    @ObservedObject var viewModel: CameraViewModel
    var onPreviewLayerCreated: (AVCaptureVideoPreviewLayer) -> Void

    func makeUIView(context: Context) -> UIView {
        let view = UIView()
        view.setContentCompressionResistancePriority(.defaultLow, for: .horizontal)
        view.setContentCompressionResistancePriority(.defaultLow, for: .vertical)
        return view
    }

    func updateUIView(_ uiView: UIView, context: Context) {
        // Only setup session and add preview layer once
        if viewModel.isPermissionGranted && uiView.layer.sublayers?.first(where: { $0 is AVCaptureVideoPreviewLayer }) == nil {
            viewModel.setupSession { previewLayer in
                guard let previewLayer = previewLayer else { return }
                // Ensure frame is set on the main thread after previewLayer is created
                previewLayer.frame = uiView.bounds 
                uiView.layer.addSublayer(previewLayer)
                onPreviewLayerCreated(previewLayer)
            }
        } else if let existingPreviewLayer = uiView.layer.sublayers?.first(where: { $0 is AVCaptureVideoPreviewLayer }) as? AVCaptureVideoPreviewLayer {
            // Update frame if view bounds change (e.g., on rotation)
             if existingPreviewLayer.frame != uiView.bounds {
                 existingPreviewLayer.frame = uiView.bounds
             }
        }
    }
}

struct ContentView: View {
    @StateObject private var cameraViewModel = CameraViewModel()
    @State private var previewLayerHolder: AVCaptureVideoPreviewLayer? // Use a more descriptive name

    var body: some View {
        ZStack {
            if cameraViewModel.isPermissionGranted {
                CameraView(viewModel: cameraViewModel, onPreviewLayerCreated: { layer in
                    self.previewLayerHolder = layer
                })
                .edgesIgnoringSafeArea(.all)
                .onAppear {
                    if !cameraViewModel.session.isRunning && self.previewLayerHolder != nil {
                        cameraViewModel.session.startRunning()
                    }
                }
                .onDisappear {
                    if cameraViewModel.session.isRunning {
                        cameraViewModel.session.stopRunning()
                    }
                }

                // Overlay for drawing detected poses and balls
                PoseOverlayView(
                    detectedJoints: cameraViewModel.detectedJoints,
                    detectedBalls: cameraViewModel.detectedBalls, // Pass detected balls
                    previewLayer: previewLayerHolder
                )

            } else {
                VStack {
                    Text("Camera permission is required to use this feature.")
                        .multilineTextAlignment(.center)
                        .padding()
                    Button("Request Permission") {
                        cameraViewModel.requestPermission()
                    }
                }
            }
        }
        .onAppear {
            if !cameraViewModel.isPermissionGranted {
                 cameraViewModel.requestPermission()
            }
        }
        .edgesIgnoringSafeArea(.all) // Apply to ZStack to ensure overlay also ignores it
    }
}

struct PoseOverlayView: View {
    var detectedJoints: [VNHumanBodyPose3DObservation.JointName: SIMD3<Float>]
    var detectedBalls: [CGRect] // Add property for detected balls
    var previewLayer: AVCaptureVideoPreviewLayer? // Pass the previewLayer for coordinate conversion

    // Define connections between joints for drawing lines
    let jointPairs: [(VNHumanBodyPose3DObservation.JointName, VNHumanBodyPose3DObservation.JointName)] = [
        (.neck, .head), (.neck, .spine), (.spine, .root),
        (.neck, .rightShoulder), (.rightShoulder, .rightElbow), (.rightElbow, .rightWrist),
        (.neck, .leftShoulder), (.leftShoulder, .leftElbow), (.leftElbow, .leftWrist),
        (.root, .rightHip), (.rightHip, .rightKnee), (.rightKnee, .rightAnkle),
        (.root, .leftHip), (.leftHip, .leftKnee), (.leftKnee, .leftAnkle)
    ]

    var body: some View {
        Canvas { context, size in
            guard previewLayer != nil else { return } // Removed unused previewLayer variable here

            // Draw detected body poses (existing logic)
            for (jointName, position3D) in detectedJoints {
                let projectedPoint = CGPoint(x: CGFloat(position3D.x + 0.5) * size.width, 
                                             y: CGFloat(-position3D.y + 0.5) * size.height) 

                context.fill(Path(ellipseIn: CGRect(x: projectedPoint.x - 5, y: projectedPoint.y - 5, width: 10, height: 10)), with: .color(.green))
                context.draw(Text("\(jointName.rawValue.capitalized) (\(String(format: "%.2f", position3D.z))m)") .font(.caption), at: projectedPoint + CGPoint(x:0, y:-10))
            }
            
            for pair in jointPairs {
                guard let joint1Pos3D = detectedJoints[pair.0],
                      let joint2Pos3D = detectedJoints[pair.1] else {
                    continue
                }
                 let p1 = CGPoint(x: CGFloat(joint1Pos3D.x + 0.5) * size.width, 
                                  y: CGFloat(-joint1Pos3D.y + 0.5) * size.height)
                 let p2 = CGPoint(x: CGFloat(joint2Pos3D.x + 0.5) * size.width, 
                                  y: CGFloat(-joint2Pos3D.y + 0.5) * size.height)
                var linePath = Path()
                linePath.move(to: p1)
                linePath.addLine(to: p2)
                context.stroke(linePath, with: .color(.red), lineWidth: 2)
            }

            // Draw detected balls
            for ballBoundingBox in detectedBalls {
                // VNRecognizedObjectObservation.boundingBox is normalized (0-1) with origin at bottom-left.
                // Convert to Canvas coordinates (origin top-left).
                let canvasRect = CGRect(
                    x: ballBoundingBox.origin.x * size.width,
                    y: (1 - ballBoundingBox.origin.y - ballBoundingBox.height) * size.height, // Flip Y and adjust for height
                    width: ballBoundingBox.width * size.width,
                    height: ballBoundingBox.height * size.height
                )
                context.stroke(Path(roundedRect: canvasRect, cornerRadius: 4), with: .color(.blue), lineWidth: 2)
                context.draw(Text("Ball").font(.caption), at: CGPoint(x: canvasRect.minX, y: canvasRect.minY - 10))
            }
        }
        .opacity(0.7) // Make overlay semi-transparent
    }
}


#Preview {
    ContentView()
}
